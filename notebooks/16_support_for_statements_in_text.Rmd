---
title: "`r gsub('.Rmd', ' ', gsub('_', ' ', knitr::current_input()))`"
author: "Holly Beale"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output:
  html_document:
    toc: true
---

```{r setup, include = FALSE}

knitr::opts_chunk$set(echo = TRUE)
# knitr::opts_knit$set(root.dir = "/data/projects/gitCode/mend_qc_publication/notebooks")

library(readr)
library(tidyr)
library(dplyr)
library(ggplot2)
library(cowplot)
library(gridGraphics)
library(RColorBrewer)
library(viridis)
library(ggrepel)

```

# Compendium stats

## revised for V9 public

General URL for all compendia: https://treehousegenomics.soe.ucsc.edu/public-data/

Name of compendium on the website: Compendium v9 Public (March 2019)

Blurb: This compendium was released in March 2019. It includes a total of 11,454 samples from Treehouse (identifiers start with "TH"), TCGA and TARGET projects. This data was generated by library preparation methods including polyA selection.

Clinical data page: https://xenabrowser.net/datapages/?dataset=TreehousePEDv9_clinical_metadata.2019-03-15.tsv&host=https%3A%2F%2Fxena.treehouse.gi.ucsc.edu%3A443

Clinical data link: https://xena.treehouse.gi.ucsc.edu/download/TreehousePEDv9_clinical_metadata.2019-03-15.tsv


### Compendium size
```{r}
v9_clinical <- read_tsv("https://xena.treehouse.gi.ucsc.edu/download/TreehousePEDv9_clinical_metadata.2019-03-15.tsv")

nrow(v9_clinical)
```

There are `r nrow(v9_clinical)` samples in the compendium 


## Threshold

```
cat /private/groups/treehouse/archive/compendium/TreehousePEDv9/compendium_info.json | grep mcs_similarity_threshold
```

```
"mcs_similarity_threshold": 0.875, 
```

# Seeds used for making subsets

```{r}

read_counts <- read_tsv("../data/read_counts.txt",
                        col_names = c("sample_id", "measurement", "read_count")) %>%
  mutate(parent_id = gsub("_est.*$", "", sample_id),
         seed = gsub("^.*seed_", "", sample_id))

```

### Not all seeds are unique across the whole experiment

```{r}

length(unique(read_counts$sample_id))
length(unique(read_counts$seed))

```

### This one is present in two parent samples

```{r}

bad_seed <- read_counts %>%
  select(parent_id, seed) %>%
  distinct %>%
  mutate(dupe = duplicated(seed)) %>%
  dplyr::filter(dupe) %>%
  pull(seed)

read_counts %>%
  dplyr::filter(seed == bad_seed)

```
  
### All seeds are unique within each parent sample

```{r}

for (this_parent_id in unique(read_counts$parent_id)) {
  print(this_parent_id)
  these_read_counts = subset(read_counts, parent_id == this_parent_id)
  print(length(unique(these_read_counts$sample_id)))
  print(length(unique(these_read_counts$seed)))
}

```

# Duration of pipelines

```{r}

timestamps_raw <- read_delim("../data/pipeline_timestamps.txt",
                             delim = " ",
                             col_names = c("sample_id",
                                           "pipeline",
                                           "start_stop",
                                           "timestamp",
                                           "deleteme")) %>%
  select(-deleteme)

library(lubridate)

timestamps <- timestamps_raw %>%
  mutate(timestamp = ymd_hms(gsub("T", " ", gsub(",", "", timestamp))),
         start_stop = gsub(":", "", start_stop)) %>%
  spread(start_stop, timestamp)

ts <- timestamps %>% 
  mutate(duration = end - start)

read_counts_for_merge <- read_counts %>%
  select(sample_id, parent_id, measurement, read_count) %>%
  spread(measurement, read_count)

ts_anno <- left_join(ts, read_counts_for_merge) %>% 
  group_by(pipeline) %>%
  mutate(minimum_pipeline_time = median(as.numeric(duration)[grepl("_est1M", sample_id)]),
         adjusted_duration = as.numeric(duration) - minimum_pipeline_time,
         min_per_M_total_reads = adjusted_duration / (total_sequences / 1e6 - 1),
         min_per_M_MEND_reads = adjusted_duration / (MEND / 1e6 - 1))

subsamples_to_select <- read_tsv("../data/evenly_selected_subsamples_with_binwidth_4.tsv")


selected_ts_anno <- ts_anno %>% filter(sample_id %in% subsamples_to_select$sample_id)

# how long does each pipeline take?
ggplot(selected_ts_anno) +
  geom_histogram(aes(x = duration, color = pipeline)) +
  facet_grid(pipeline~parent_id) +
  scale_color_brewer(palette = "Set1") +
  theme(axis.text.x = element_text(angle = 90, hjust = 1, vjust = 0.5))

# how variable is the rate per M reads
ggplot(selected_ts_anno) +
  geom_histogram(aes(x = min_per_M_total_reads, fill = parent_id)) +
  facet_grid(~pipeline) +
  scale_fill_brewer(palette = "Set1") +
  theme(axis.text.x = element_text(angle = 90, hjust = 1, vjust = 0.5))

# how variable is the rate per M MEND reads
ggplot(selected_ts_anno) +
  geom_histogram(aes(x = min_per_M_MEND_reads, fill = parent_id)) +
  facet_grid(~pipeline) +
  scale_fill_brewer(palette = "Set1") +
  theme(axis.text.x = element_text(angle = 90, hjust = 1, vjust = 0.5))

ts_summary <- selected_ts_anno %>%
  group_by(pipeline, minimum_pipeline_time) %>%
  summarize(median_min_per_M_total_reads = median(min_per_M_total_reads),
            median_min_per_M_MEND_reads = median(min_per_M_MEND_reads),
            sd_min_per_M_total_reads = sd(min_per_M_total_reads),
            sd_min_per_M_MEND_reads = sd(min_per_M_MEND_reads)) %>%
  select(pipeline,
         minimum_pipeline_time,
         median_min_per_M_total_reads,
         sd_min_per_M_total_reads,
         median_min_per_M_MEND_reads,
         sd_min_per_M_MEND_reads)

minutes_per_expression_sample_base=selected_ts_anno %>%
  dplyr::filter(pipeline == "expression") %>%
  pull(minimum_pipeline_time) %>%
  unique %>%
  round(1) 

minutes_per_million_reads_in_expression_sample <- ts_summary %>%
  dplyr::filter(pipeline == "expression") %>%
  pull(median_min_per_M_total_reads) %>%
  unique %>%
  round(1)

minutes_per_qc_sample_base <- selected_ts_anno %>%
  dplyr::filter(pipeline == "umend_qc") %>%
  pull(minimum_pipeline_time) %>%
  unique %>%
  round(1) 

minutes_per_million_reads_in_qc_sample <- ts_summary %>%
  dplyr::filter(pipeline == "umend_qc") %>%
  pull(median_min_per_M_total_reads) %>%
  unique %>%
  round(1)

example_total_reads <- 70

estimate_expression_pipeline_duration <-
  (minutes_per_expression_sample_base-minutes_per_million_reads_in_expression_sample) +
  example_total_reads * minutes_per_million_reads_in_expression_sample

estimate_mend_qc_pipeline_duration <-
  (minutes_per_qc_sample_base-minutes_per_million_reads_in_qc_sample) +
  example_total_reads * minutes_per_million_reads_in_qc_sample

```

# Statements about duration of pipelines

```{r}
computing_requirements_statement <- paste("Based on the processing times of",

length(unique(selected_ts_anno$sample_id)),

"on computers with 64GB of memory and 12 VCPUs, the expression quantification pipeline has a minimum duration of",

minutes_per_expression_sample_base-minutes_per_million_reads_in_expression_sample,

"minutes, and processing each million total reads requires, on average, an additional",

minutes_per_million_reads_in_expression_sample,

"minutes. The MEND QC pipeline has a minimum duration of",

minutes_per_qc_sample_base-minutes_per_million_reads_in_qc_sample,
 
 "minutes for every RNA-Seq sample plus",
minutes_per_million_reads_in_qc_sample,
"minutes per million total reads present in the data. For a sample with",
      example_total_reads,
      "million total reads, the expression pipeline would be predicted to take to take",
       round(estimate_expression_pipeline_duration),
      "minutes (",
       round(estimate_expression_pipeline_duration/60,1),
      "hours). Adding MEND QC would increase the duration by",
      round(estimate_mend_qc_pipeline_duration),
      "minutes (",
      round(estimate_mend_qc_pipeline_duration/60,1),
      "hours), increasing the total time by",
      round(100*estimate_mend_qc_pipeline_duration/estimate_expression_pipeline_duration),
      "percent relative to running only the expression pipeline")

print(gsub("\\( ", "(", computing_requirements_statement))
 
```



## dynamically generated result
Based on the processing times of `r nrow(timestamps_raw)/8` subsamples, the expression quantification pipeline takes `r minutes_per_expression_sample_base-minutes_per_million_reads_in_expression_sample` minutes for every RNA-Seq sample plus `r minutes_per_million_reads_in_expression_sample ` minutes per million total reads present in the data. 

The MEND qc quantification pipeline takes `r minutes_per_qc_sample_base-minutes_per_million_reads_in_qc_sample` minutes for every RNA-Seq sample plus `r minutes_per_million_reads_in_qc_sample ` minutes per million total reads present in the data. 

For a sample with `r example_total_reads` M total reads as recommended above, the expression pipeline would be predicted to take  to take `r round(estimate_expression_pipeline_duration)` minutes (`r round(estimate_expression_pipeline_duration/60,1)` hours). To add MEND qc would increase the duration by `r round(estimate_mend_qc_pipeline_duration)` minutes  (`r round(estimate_mend_qc_pipeline_duration/60,1)` hours). 

We give this number in total reads because the number of MEND reads can't be known until the pipeline is run, and is therefore not available for estimating how long the pipeline will take. 

Note to self: consider adding SD



## previous (static) result
Based on the processing times of 65 subsamples on computers with 64GB of memory and 12 VCPUs, the expression quantification pipeline has a minimum duration of 28.7 minutes, and processing each million total reads requires, on average, an additional 6.4 minutes. The MEND QC pipeline has a minimum duration of 1.7 minutes plus 2 minutes per million total reads present in the data. For a sample with 70 M total reads, the expression pipeline would be predicted to take to take 515 minutes (8.6 hours). Adding MEND QC would increase the duration by 154 minutes (2.6 hours).


# Computer size

Specs
Flavor Name
m1.large
Flavor ID
1c52ce9b-9e54-4a3b-b851-556382e733e4
RAM
64GB
VCPUs
12 VCPU
Disk
40GB
Ephemeral Disk
2048GB

# Number of genes used for correlation

```
cat /Users/hbeale/Documents/Dropbox/ucsc/projects/new\ qc\ samples\ -\ working/TH_Eval010/outlier_results_TH_Eval_010 | cut -f8 | sort |  uniq -c
  27124 
  31457 pc_dropped
      1 pc_is_filtered
```
